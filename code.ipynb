{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "842121bd",
   "metadata": {},
   "source": [
    "# **Import necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6aa8cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ynaya\\OneDrive\\Documents\\RAG Assistant\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, TextLoader\n",
    "from langchain_text_splitters import Language, RecursiveCharacterTextSplitter\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames\n",
    "from langchain_ibm import WatsonxEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from langchain.chains import RetrievalQA\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe65ed8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4544259c",
   "metadata": {},
   "source": [
    "# **Task 1: Load document using LangChain for different sources**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8e9f417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF URL\n",
    "pdf_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WgM1DaUn2SYPcCg_It57tA/A-Comprehensive-Review-of-Low-Rank-Adaptation-in-Large-Language-Models-for-Efficient-Parameter-Tuning-1.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bd9a0c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WgM1DaUn2SYPcCg_It57tA/A-Comprehensive-Review-of-Low-Rank-Adaptation-in-Large-Language-Models-for-Efficient-Parameter-Tuning-1.pdf', 'file_path': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WgM1DaUn2SYPcCg_It57tA/A-Comprehensive-Review-of-Low-Rank-Adaptation-in-Large-Language-Models-for-Efficient-Parameter-Tuning-1.pdf', 'page': 0, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'TeX', 'producer': 'pdfTeX-1.40.26', 'creationDate': 'D:20240910215042Z', 'modDate': 'D:20240910215042Z', 'trapped': ''}, page_content='A Comprehensive Review of Low-Rank\\nAdaptation in Large Language Models for\\nEfficient Parameter Tuning\\nSeptember 10, 2024\\nAbstract\\nNatural Language Processing (NLP) often involves pre-training large\\nmodels on extensive datasets and then adapting them for specific tasks\\nthrough fine-tuning. However, as these models grow larger, like GPT-3\\nwith 175 billion parameters, fully fine-tuning them becomes computa-\\ntionally expensive. We propose a novel method called LoRA (Low-Rank\\nAdaptation) that significantly reduces the overhead by freezing the orig-\\ninal model weights and only training small rank decomposition matrices.\\nThis leads to up to 10,000 times fewer trainable parameters and reduces\\nGPU memory usage by three times. LoRA not only maintains but some-\\ntimes surpasses fine-tuning performance on models like RoBERTa, De-\\nBERTa, GPT-2, and GPT-3.\\nUnlike other methods, LoRA introduces\\nno extra latency during inference, making it more efficient for practical\\napplications.\\nAll relevant code and model checkpoints are available at\\nhttps://github.com/microsoft/LoRA.\\n1\\nIntroduction\\nMany natural language processing (NLP) applications rely on adapting large,\\npre-trained language models for various downstream tasks. Typically, this is\\ndone through fine-tuning, where all the parameters of the pre-trained model are\\nupdated. However, a significant drawback of fine-tuning is that the adapted\\nmodel has just as many parameters as the original one. As models grow in size,\\nwhat was once a manageable issue for models like GPT-2 or RoBERTa large\\nbecomes a serious deployment challenge with larger models like GPT-3, which\\nhas 175 billion trainable parameters.\\nTo mitigate these challenges, researchers have explored adapting only cer-\\ntain parts of the model or adding external modules specific to each task. This\\napproach reduces the need to store and manage large numbers of parameters\\nfor each task, greatly improving efficiency during deployment. However, cur-\\nrent methods often introduce drawbacks, such as inference delays by increasing\\n1\\n'),\n",
       " Document(metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WgM1DaUn2SYPcCg_It57tA/A-Comprehensive-Review-of-Low-Rank-Adaptation-in-Large-Language-Models-for-Efficient-Parameter-Tuning-1.pdf', 'file_path': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WgM1DaUn2SYPcCg_It57tA/A-Comprehensive-Review-of-Low-Rank-Adaptation-in-Large-Language-Models-for-Efficient-Parameter-Tuning-1.pdf', 'page': 1, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'TeX', 'producer': 'pdfTeX-1.40.26', 'creationDate': 'D:20240910215042Z', 'modDate': 'D:20240910215042Z', 'trapped': ''}, page_content='model depth or reducing the usable sequence length. Furthermore, these meth-\\nods typically do not perform as well as full fine-tuning, leading to a trade-off\\nbetween efficiency and model performance.\\nInspired by prior works that demonstrate over-parametrized models often\\nreside in a low intrinsic dimensional space, we hypothesize that weight changes\\nduring model adaptation also have a low “intrinsic rank.” This insight leads to\\nour Low-Rank Adaptation (LoRA) approach. LoRA optimizes low-rank decom-\\nposition matrices for the dense layers’ weight changes during adaptation, while\\nkeeping the pre-trained weights frozen. As illustrated in Figure 1, even with\\nlarge models like GPT-3 (with up to 12,288 dimensions in full rank), a low-rank\\nmatrix (rank 1 or 2) is sufficient, making LoRA highly efficient in terms of both\\nstorage and computation.\\nLoRA has several notable advantages:\\n• The pre-trained model can be shared, and small LoRA modules can be\\ncreated for various tasks. By freezing the main model and only switching\\nthe matrices A and B (shown in Figure 1), storage and task-switching\\noverhead are significantly reduced.\\n• LoRA improves training efficiency and reduces hardware requirements,\\nlowering the entry barrier by up to threefold when using adaptive opti-\\nmizers. This is because LoRA only requires updating the smaller low-rank\\nmatrices, avoiding the need to calculate gradients for most parameters.\\n• The simple linear design allows merging of the trainable matrices with\\nthe frozen pre-trained weights during deployment, ensuring no additional\\ninference latency compared to fully fine-tuned models.\\n• LoRA is compatible with many existing methods and can be combined\\nwith approaches like prefix-tuning.\\nIn this work, we follow standard conventions for Transformer architecture\\nand refer to dimensions such as dmodel, and projection matrices like Wq, Wk, Wv,\\nand Wo for the self-attention module. W or W0 represents a pre-trained weight\\nmatrix, while ∆W refers to its update during adaptation. The rank r denotes\\nthe rank of a LoRA module. Throughout, we use Adam for optimization and\\nmaintain the Transformer MLP feedforward dimension as dffn = 4 × dmodel.\\n1.1\\nKey Advantages of LoRA\\n• Efficient Task Switching: A pre-trained model can support multiple\\ntasks by swapping the small LoRA matrices, reducing storage needs.\\n• Reduced Hardware Requirements: LoRA lowers the GPU memory\\nneeded for training by freezing most parameters and only training the\\nlow-rank matrices.\\n• No Additional Latency: LoRA incurs no extra inference delay because\\nthe matrices can be merged with the pre-trained weights when deployed.\\n2\\n'),\n",
       " Document(metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WgM1DaUn2SYPcCg_It57tA/A-Comprehensive-Review-of-Low-Rank-Adaptation-in-Large-Language-Models-for-Efficient-Parameter-Tuning-1.pdf', 'file_path': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WgM1DaUn2SYPcCg_It57tA/A-Comprehensive-Review-of-Low-Rank-Adaptation-in-Large-Language-Models-for-Efficient-Parameter-Tuning-1.pdf', 'page': 2, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'TeX', 'producer': 'pdfTeX-1.40.26', 'creationDate': 'D:20240910215042Z', 'modDate': 'D:20240910215042Z', 'trapped': ''}, page_content='• Combining with Other Methods: LoRA can be used with other ap-\\nproaches, like prefix-tuning, to further optimize model performance.\\n2\\nProblem Statement\\nAlthough our approach is independent of the specific training objective, we focus\\non language modeling as the central application.\\nBelow, we outline the key\\naspects of the language modeling problem, particularly the goal of maximizing\\nconditional probabilities based on task-specific prompts.\\nAssume we have an autoregressive language model PΦ(y|x) that is pre-\\ntrained and parameterized by Φ.\\nFor example, PΦ(y|x) could be a general\\nmulti-task model such as GPT, built on top of the Transformer architecture.\\nThe model can then be adapted to different downstream tasks such as text\\nsummarization, machine reading comprehension (MRC), and natural language\\nto SQL (NL2SQL). Each downstream task is represented as a training set of\\ncontext-output pairs:\\nZ = {(xi, yi)}i=1,...,N,\\nwhere both xi and yi are sequences of tokens. For instance, in NL2SQL, xi\\nmight represent a natural language question and yi would be the corresponding\\nSQL query; in summarization, xi represents the article and yi would be its\\nsummary.\\nIn traditional fine-tuning, the model is initialized using the pre-trained weights\\nΦ0, which are then updated to Φ0 + ∆Φ by optimizing the model’s parameters\\nto maximize the conditional probabilities for each token:\\nmax\\nΦ\\nX\\n(x,y)∈Z\\n|y|\\nX\\nt=1\\nlog (PΦ(yt|x, y<t))\\n(1)\\nA significant limitation of full fine-tuning is that for every downstream task, a\\ndifferent set of parameters ∆Φ must be learned, and the size of ∆Φ is equal to the\\nsize of Φ0. For large models, such as GPT-3 with 175 billion parameters, storing\\nand deploying multiple instances of fine-tuned models becomes impractical or\\nextremely challenging.\\nTo address this issue, we propose a more efficient approach where the task-\\nspecific parameter updates ∆Φ = ∆Φ(Θ) are encoded using a much smaller set\\nof parameters Θ, where |Θ| ≪|Φ0|. As a result, optimizing the model for each\\ntask reduces to optimizing Θ as follows:\\nmax\\nΘ\\nX\\n(x,y)∈Z\\n|y|\\nX\\nt=1\\nlog\\n\\x00pΦ0+∆Φ(Θ)(yt|x, y<t)\\n\\x01\\n(2)\\nIn the following sections, we explore a low-rank approach for representing\\n∆Φ, making the adaptation process more efficient in both computational and\\nmemory terms.\\nFor large models like GPT-3 175B, this method allows the\\ntrainable parameters |Θ| to be reduced to as little as 0.01% of |Φ0|.\\n3\\n'),\n",
       " Document(metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WgM1DaUn2SYPcCg_It57tA/A-Comprehensive-Review-of-Low-Rank-Adaptation-in-Large-Language-Models-for-Efficient-Parameter-Tuning-1.pdf', 'file_path': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WgM1DaUn2SYPcCg_It57tA/A-Comprehensive-Review-of-Low-Rank-Adaptation-in-Large-Language-Models-for-Efficient-Parameter-Tuning-1.pdf', 'page': 3, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'TeX', 'producer': 'pdfTeX-1.40.26', 'creationDate': 'D:20240910215042Z', 'modDate': 'D:20240910215042Z', 'trapped': ''}, page_content='3\\nLimitations on Current Solutions\\nThe challenge we aim to address is not new. Since the rise of transfer learning,\\na great deal of work has focused on making model adaptation more efficient in\\nterms of both parameters and computation. For an overview, see Section 6 for\\nsome well-known works. Focusing on language modeling, two prominent strate-\\ngies for efficient adaptation stand out: adding adapter layers, or optimizing the\\ninput layer activations. However, both approaches come with limitations, espe-\\ncially when applied in large-scale, latency-sensitive production environments.\\n3.1\\nAdapter Layers and Inference Latency\\nThere are many variations of adapters. We focus on the original adapter design\\nfrom [?], which introduces two adapter layers per Transformer block, and a\\nmore recent approach by [?], which only uses one adapter per block but with an\\nadditional LayerNorm [?]. Although overall latency can be reduced by pruning\\nlayers or leveraging multi-task settings [?], [?], there is no way to completely\\neliminate the additional computation introduced by adapter layers. This might\\nseem minor since adapters generally have few parameters (typically less than\\n1% of the original model) due to their small bottleneck dimension, which limits\\nthe number of floating-point operations (FLOPs). However, large-scale neural\\nnetworks rely heavily on parallel processing to maintain low latency, and adapter\\nlayers are processed sequentially. This becomes more evident in scenarios with\\nlow batch sizes, such as real-time inference, where models like GPT-2 [?] running\\non a single GPU experience noticeable increases in latency, even with small\\nbottleneck dimensions (Table 1).\\nThe issue is further compounded when models need to be sharded across\\nmultiple devices, since the increased model depth requires more synchronous\\nGPU operations like AllReduce and Broadcast, unless adapter parameters are\\nredundantly replicated.\\n3.2\\nChallenges with Directly Optimizing the Prompt\\nAnother approach, such as prefix tuning [?], faces a different challenge. We\\nhave observed that prefix tuning is often difficult to optimize and that its per-\\nformance does not consistently improve as more trainable parameters are added,\\nconfirming earlier findings. Moreover, allocating part of the sequence length for\\nadaptation inevitably reduces the available sequence length for processing task-\\nrelated data, which seems to hinder prompt tuning’s performance compared to\\nother methods. We will further explore this issue in Section 5.\\n4\\n'),\n",
       " Document(metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WgM1DaUn2SYPcCg_It57tA/A-Comprehensive-Review-of-Low-Rank-Adaptation-in-Large-Language-Models-for-Efficient-Parameter-Tuning-1.pdf', 'file_path': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WgM1DaUn2SYPcCg_It57tA/A-Comprehensive-Review-of-Low-Rank-Adaptation-in-Large-Language-Models-for-Efficient-Parameter-Tuning-1.pdf', 'page': 4, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'TeX', 'producer': 'pdfTeX-1.40.26', 'creationDate': 'D:20240910215042Z', 'modDate': 'D:20240910215042Z', 'trapped': ''}, page_content='Batch Size\\nSequence Length\\n|Θ|\\nLatency (ms)\\nFine-Tune/LoRA\\n32\\n512\\n0.5M\\n1449.4 ± 0.8\\n16\\n256\\n11M\\n338.0 ± 0.6\\n1\\n128\\n11M\\n19.8 ± 2.7\\nAdapterL\\n32\\n512\\n0.5M\\n1482.0 ± 1.0 (+2.2%)\\n16\\n256\\n11M\\n354.8 ± 0.5 (+5.0%)\\n1\\n128\\n11M\\n23.9 ± 2.1 (+20.7%)\\nAdapterH\\n32\\n512\\n0.5M\\n1492.2 ± 1.0 (+3.0%)\\n16\\n256\\n11M\\n366.3 ± 0.5 (+8.4%)\\n1\\n128\\n11M\\n25.8 ± 2.2 (+30.3%)\\nTable 1: Inference latency of a forward pass in GPT-2 Medium measured over\\n100 trials using an NVIDIA Quadro RTX8000. ”|Θ|” refers to the number of\\ntrainable parameters in the adapter layers. AdapterL and AdapterH are two\\ntypes of adapter tuning. The impact on latency becomes significant, particularly\\nin online scenarios with shorter sequences and smaller batch sizes.\\n4\\nOur Method\\nIn this section, we explain the structure of LoRA and its practical benefits.\\nThe principles outlined here apply generally to dense layers in neural networks,\\nalthough we focus on specific weights in Transformer language models, as these\\nmodels serve as the central example in our experiments.\\n4.1\\nLow-Rank Parameterized Update Matrices\\nNeural networks contain numerous dense layers that perform matrix multipli-\\ncation, and the weight matrices in these layers typically have a full rank. When\\nadapting to a particular task, it shows that pre-trained language models pos-\\nsess a low ”intrinsic dimension” and can still perform effectively after a random\\nprojection to a smaller subspace. Drawing inspiration from this, we hypothesize\\nthat updates to the weights during adaptation also have a low ”intrinsic rank.”\\nFor a pre-trained weight matrix W0 ∈Rd×k, we limit its update by express-\\ning it as a low-rank decomposition, W0 + ∆W = W0 + BA, where B ∈Rd×r,\\nA ∈Rr×k, and the rank r ≪min(d, k). During training, W0 is fixed, and A\\nand B are the trainable parameters. Both W0 and ∆W = BA are multiplied\\nwith the input, and their respective outputs are summed element-wise. Thus,\\nfor h = W0x, our updated forward pass becomes:\\nh = W0x + ∆Wx = W0x + BAx\\nWe illustrate this reparametrization in Figure 1. We initialize A with random\\nGaussian values and set B to zero, meaning ∆W = BA is zero at the start\\n5\\n'),\n",
       " Document(metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WgM1DaUn2SYPcCg_It57tA/A-Comprehensive-Review-of-Low-Rank-Adaptation-in-Large-Language-Models-for-Efficient-Parameter-Tuning-1.pdf', 'file_path': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WgM1DaUn2SYPcCg_It57tA/A-Comprehensive-Review-of-Low-Rank-Adaptation-in-Large-Language-Models-for-Efficient-Parameter-Tuning-1.pdf', 'page': 5, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'TeX', 'producer': 'pdfTeX-1.40.26', 'creationDate': 'D:20240910215042Z', 'modDate': 'D:20240910215042Z', 'trapped': ''}, page_content='of training. We then scale ∆Wx by α\\nr , where α is a constant dependent on\\nr.\\nWhen using Adam for optimization, adjusting α has an effect similar to\\ntuning the learning rate. Therefore, we use the same α for our first experiments\\nand avoid tuning it. This scaling method also minimizes the need to adjust\\nhyperparameters when varying r.\\n4.1.1\\nA Generalization of Full Fine-Tuning\\nA more general fine-tuning technique involves training only a subset of pre-\\ntrained parameters. LoRA extends this approach by eliminating the need for\\nfull-rank gradient updates to weight matrices. Instead, LoRA uses low-rank\\nmatrices for adaptation.\\nIf LoRA is applied to all weight matrices, and all\\nbiases are trained, the expressiveness of full fine-tuning is recovered by setting\\nthe LoRA rank r equal to the rank of the pre-trained weight matrices. As the\\nnumber of trainable parameters increases, LoRA approaches the full fine-tuning\\nperformance, while adapter-based techniques converge to simpler models that\\ncannot process long input sequences.\\n4.1.2\\nNo Additional Inference Latency\\nWhen deploying LoRA, we can explicitly compute W = W0 + BA and use it\\nduring inference. This means that when switching between tasks, we can quickly\\nsubtract BA and add a different low-rank matrix B′A′ without consuming ex-\\ntra memory. This ensures that no additional inference latency is introduced\\ncompared to fully fine-tuned models.\\n4.2\\nApplying LoRA to Transformer Models\\nIn principle, LoRA can be applied to any subset of weight matrices in a neural\\nnetwork to minimize the number of trainable parameters.\\nIn a Transformer\\narchitecture, the self-attention module contains four projection matrices Wq,\\nWk, Wv, and Wo, and the MLP module contains two more matrices. We treat\\nthe weight matrices in the self-attention module as a single dmodel × dmodel\\nmatrix, despite them being split into different attention heads.\\nTo simplify\\nthe process and improve parameter efficiency, we restrict our method to only\\nadapting the attention weights for downstream tasks, leaving the MLP module\\nfrozen. The effect of adapting various attention weight matrices in a Transformer\\nis further explored in Section 7.1. We leave the investigation of adapting MLP\\nlayers, LayerNorm layers, and biases to future work.\\n4.2.1\\nPractical Benefits and Limitations\\nOne of the major advantages of LoRA is its reduction in memory and storage\\ncosts. For large Transformer models using Adam, LoRA can cut VRAM usage\\nby up to two-thirds if r ≪dmodel, since it eliminates the need to store optimizer\\nstates for frozen parameters. For example, with GPT-3 175B, VRAM usage\\nduring training drops from 1.2 TB to 350 GB. With r = 4, and only the query\\n6\\n'),\n",
       " Document(metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WgM1DaUn2SYPcCg_It57tA/A-Comprehensive-Review-of-Low-Rank-Adaptation-in-Large-Language-Models-for-Efficient-Parameter-Tuning-1.pdf', 'file_path': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WgM1DaUn2SYPcCg_It57tA/A-Comprehensive-Review-of-Low-Rank-Adaptation-in-Large-Language-Models-for-Efficient-Parameter-Tuning-1.pdf', 'page': 6, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'TeX', 'producer': 'pdfTeX-1.40.26', 'creationDate': 'D:20240910215042Z', 'modDate': 'D:20240910215042Z', 'trapped': ''}, page_content='and value matrices being adapted, the checkpoint size decreases by approxi-\\nmately 10,000× (from 350 GB to 35 MB). This makes it possible to train using\\nsignificantly fewer GPUs and avoid I/O bottlenecks. LoRA also enables easier\\ntask-switching during deployment by simply swapping out the LoRA weights,\\nwhich requires far less memory than loading entirely new model parameters.\\nAdditionally, LoRA offers a 25% training speedup compared to full fine-tuning\\nbecause there is no need to compute gradients for most parameters.\\nHowever, LoRA does have some limitations.\\nIt is not straightforward to\\ncombine multiple tasks with different low-rank matrices A and B in a single\\nforward pass if BA is absorbed into W to remove additional inference latency.\\nWhile it is possible to dynamically select LoRA modules during inference, this\\nsolution is not suitable for scenarios where low-latency responses are crucial.\\n5\\nEmpirical Experiments\\nWe assess LoRA’s performance in downstream tasks across several models in-\\ncluding RoBERTa, DeBERTa, and GPT-2, before scaling up to GPT-3 175B.\\nOur experiments cover various tasks, ranging from natural language understand-\\ning (NLU) to natural language generation (NLG). For RoBERTa and DeBERTa,\\nwe evaluate on the GLUE benchmark. All experiments were performed using\\nNVIDIA Tesla V100 GPUs.\\n5.1\\nBaselines\\nFor comparison with a wide range of baselines, we replicate experimental setups\\nfrom previous studies and, where possible, reuse reported results. This might\\nresult in some baselines being present in only a subset of experiments.\\nFine-Tuning (FT) is a common method for adapting models. During fine-\\ntuning, the model’s pre-trained weights and biases are updated using gradient\\ndescent. A variant of this is fine-tuning only select layers, while freezing the\\nrest. One such baseline from prior work on GPT-2 updates only the last two\\nlayers (denoted as FTTop2).\\nBitFit is another baseline in which only the bias parameters are updated,\\nwhile all other parameters remain frozen. This method has gained attention,\\nincluding in recent studies [?].\\nPrefix-embedding tuning (PreEmbed) involves adding special tokens to\\nthe input sequence, and training their embeddings. These tokens do not belong\\nto the model’s original vocabulary. Their placement—either prepended (prefix)\\nor appended (infix)—can significantly affect performance, as highlighted in [?].\\nPrefix-layer tuning (PreLayer) extends prefix tuning by learning train-\\nable activations at each Transformer layer. This results in a larger number of\\ntrainable parameters, as activations from prior layers are progressively replaced.\\nThe total number of trainable parameters is given by |Θ| = L×dmodel ×(lp +li),\\nwhere L is the number of Transformer layers.\\n7\\n'),\n",
       " Document(metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WgM1DaUn2SYPcCg_It57tA/A-Comprehensive-Review-of-Low-Rank-Adaptation-in-Large-Language-Models-for-Efficient-Parameter-Tuning-1.pdf', 'file_path': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WgM1DaUn2SYPcCg_It57tA/A-Comprehensive-Review-of-Low-Rank-Adaptation-in-Large-Language-Models-for-Efficient-Parameter-Tuning-1.pdf', 'page': 7, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'TeX', 'producer': 'pdfTeX-1.40.26', 'creationDate': 'D:20240910215042Z', 'modDate': 'D:20240910215042Z', 'trapped': ''}, page_content='Adapter tuning [?] introduces additional fully connected adapter layers\\nbetween existing layers in the Transformer.\\nSeveral variants exist, such as\\nAdapterH and AdapterL [?], which differ in the placement of adapters within\\nthe network. The number of trainable parameters in these methods is |Θ| =\\nLAdpt × (2 × dmodel × r + r + dmodel) + 2 × LLN × dmodel.\\nLoRA, on the other hand, introduces trainable low-rank matrices to the ex-\\nisting weight matrices. As detailed in Section 4.2, LoRA is applied to the query\\nand value matrices in most experiments. The number of trainable parameters\\nis determined by the rank r and the shape of the original weight matrices:\\n|Θ| = 2 × LLoRA × dmodel × r, where LLoRA represents the number of weight\\nmatrices to which LoRA is applied.\\nTable 2: GPT-2 Medium and Large results on E2E NLG Challenge. Higher\\nscores are better for all metrics. Confidence intervals are provided for experi-\\nments we conducted. *Results from prior work.\\nModel & Method\\n# Trainable Parameters\\nBLEU\\nNIST\\nMET\\nROUGE-L\\nCIDEr\\nGPT-2 M (FT)*\\n354.92M\\n68.2\\n8.62\\n46.2\\n71.0\\n2.47\\nGPT-2 M (LoRA)\\n0.35M\\n70.4 ± 0.1\\n8.85 ± 0.2\\n46.8 ± 0.2\\n71.8 ± 0.1\\n2.53 ± 0.2\\nGPT-2 L (FT)*\\n774.03M\\n68.5\\n8.78\\n46.0\\n69.9\\n2.45\\nGPT-2 L (LoRA)\\n0.77M\\n70.4 ± 0.1\\n8.89 ± 0.2\\n46.8 ± 0.2\\n72.0 ± 0.2\\n2.47 ± 0.2\\n5.2\\nScaling LoRA to GPT-3 175B\\nTo further test LoRA’s scalability, we apply it to GPT-3 175B. Given the large\\ncomputational cost of GPT-3, we only report standard deviations for each task\\nbased on multiple random seeds. See Appendix D.4 for hyperparameters used.\\nAs presented in Table ??, LoRA matches or outperforms full fine-tuning on\\nWikiSQL, MultiNLI, and SAMSum. Notably, we observe that certain methods\\ndo not consistently benefit from increasing the number of trainable parame-\\nters. As shown in Figure 1, LoRA remains efficient even at low ranks, avoiding\\nthe performance degradation seen with larger token embeddings in prefix-based\\nmethods.\\nFigure 1: GPT-3 175B validation accuracy vs. the number of trainable parame-\\nters for several adaptation methods on WikiSQL and MNLI. LoRA demonstrates\\nbetter scalability and performance.\\n8\\n'),\n",
       " Document(metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WgM1DaUn2SYPcCg_It57tA/A-Comprehensive-Review-of-Low-Rank-Adaptation-in-Large-Language-Models-for-Efficient-Parameter-Tuning-1.pdf', 'file_path': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WgM1DaUn2SYPcCg_It57tA/A-Comprehensive-Review-of-Low-Rank-Adaptation-in-Large-Language-Models-for-Efficient-Parameter-Tuning-1.pdf', 'page': 8, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'TeX', 'producer': 'pdfTeX-1.40.26', 'creationDate': 'D:20240910215042Z', 'modDate': 'D:20240910215042Z', 'trapped': ''}, page_content='6\\nRelated Works\\n6.1\\nTransformer Language Models\\nThe Transformer architecture, as introduced by Vaswani et al.\\n(2017), has\\nproven to be a highly effective sequence-to-sequence model due to its heavy use\\nof self-attention mechanisms. Radford et al. (2018) applied it to autoregressive\\nlanguage modeling, significantly boosting its utility in the field.\\nSince then,\\nTransformer-based models have become a staple in natural language processing\\n(NLP), achieving state-of-the-art results in a wide variety of tasks. Notably,\\nBERT (Devlin et al., 2018) and GPT-2 (Radford et al., 2019) have paved the\\nway for large-scale pre-trained language models that, when fine-tuned, deliver\\nexcellent performance on specific tasks. The next breakthrough came with GPT-\\n3 (Brown et al., 2020), which is currently the largest single Transformer language\\nmodel with 175 billion parameters.\\n6.2\\nPrompt Engineering and Fine-Tuning\\nDespite GPT-3’s ability to adapt its behavior with minimal data (few-shot learn-\\ning), its performance is highly sensitive to how the input prompt is structured\\n(Brown et al., 2020). This has led to the rise of ”prompt engineering,” a process\\nthat involves crafting and fine-tuning the input prompts to maximize model per-\\nformance on specific tasks. Fine-tuning, on the other hand, refers to retraining\\na model pre-trained on general domains to adapt it to a particular task (Devlin\\net al., 2018; Radford et al., 2018). Some approaches only update a subset of the\\nmodel’s parameters (Collobert and Weston, 2008), but it is common practice to\\nfine-tune all parameters to achieve the best performance. However, performing\\nfull fine-tuning on a model as large as GPT-3, with its 175 billion parameters,\\nposes significant challenges due to the large memory requirements and the com-\\nputational resources needed, making it as resource-intensive as pre-training.\\n6.3\\nParameter-Efficient Adaptation\\nMany techniques have been developed to address the inefficiency of full fine-\\ntuning by adapting only certain layers or introducing adapter modules. Houlsby\\net al. (2019), Rebuffi et al. (2017), and Lin et al. (2020) proposed inserting\\nadapter layers between existing layers in the network. These adapters allow for\\nparameter-efficient adaptation by learning only a small number of task-specific\\nparameters. Our method imposes a low-rank constraint on the weight updates,\\nensuring that learned weights can be merged with the main model weights dur-\\ning inference, thus introducing no additional latency, unlike the adapter layers.\\nA related approach, COMPACTER (Mahabadi et al., 2021), uses Kronecker\\nproducts to parametrize the adapters, further improving parameter efficiency.\\nAdditionally, prompt optimization techniques, such as those proposed by Li and\\nLiang (2021), Lester et al. (2021), and Hambardzumyan et al. (2020), aim to\\noptimize the input tokens directly. However, these approaches typically reduce\\n9\\n'),\n",
       " Document(metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WgM1DaUn2SYPcCg_It57tA/A-Comprehensive-Review-of-Low-Rank-Adaptation-in-Large-Language-Models-for-Efficient-Parameter-Tuning-1.pdf', 'file_path': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WgM1DaUn2SYPcCg_It57tA/A-Comprehensive-Review-of-Low-Rank-Adaptation-in-Large-Language-Models-for-Efficient-Parameter-Tuning-1.pdf', 'page': 9, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'TeX', 'producer': 'pdfTeX-1.40.26', 'creationDate': 'D:20240910215042Z', 'modDate': 'D:20240910215042Z', 'trapped': ''}, page_content='the available sequence length for task processing. Our work can be combined\\nwith such methods for further gains in efficiency.\\n6.4\\nLow-Rank Structures in Deep Learning\\nLow-rank structures are prevalent in many machine learning problems, and sev-\\neral studies have explored imposing these constraints on deep models. Li et\\nal. (2016), Cai et al. (2010), and Grasedyck et al. (2013) showed that many\\nlearning tasks have an intrinsic low-rank structure. For deep neural networks,\\nparticularly over-parameterized models, it has been shown that they often ex-\\nhibit low-rank properties after training (Oymak et al., 2019). Prior works, such\\nas those by Sainath et al. (2013), Zhang et al. (2014), and Denil et al. (2014),\\nhave explicitly imposed low-rank constraints during training to enhance model\\nefficiency. However, our approach differs in that we apply low-rank updates to\\nfrozen pre-trained models, making it highly effective for task-specific adapta-\\ntion. Neural networks with low-rank structures have been shown to outperform\\nclassical methods such as finite-width neural tangent kernels (Allen-Zhu et al.,\\n2019; Li and Liang, 2018), and low-rank adaptations are particularly useful in\\nadversarial training scenarios (Allen-Zhu and Li, 2020). This makes our pro-\\nposed low-rank adaptation well-grounded in both theory and practice.\\n7\\nAnalyzing Low-Rank Adaptations\\nIn light of the demonstrated benefits of LoRA, we aim to further explore the\\nattributes of low-rank adaptation as applied to various downstream tasks. The\\nlow-rank structure does not only reduce the hardware requirements for conduct-\\ning parallel experiments, but it also provides better insight into how adapted\\nweights align with pre-trained weights. Our focus lies on GPT-3 175B, where\\nwe managed to significantly reduce the number of trainable parameters (up to\\n10,000×) without sacrificing task performance.\\nIn this section, we address some key questions:\\n• 1) With a constrained parameter budget, which weight matrices should\\nbe adapted to achieve the best downstream task performance?\\n• 2) Is the adapted matrix ∆W truly rank-deficient, and if so, what rank is\\noptimal for practical use?\\n• 3) How is ∆W related to the pre-trained weights W? Does ∆W exhibit\\nhigh correlation with W, and what is the comparative size of ∆W to W?\\nThe answers to these questions provide valuable insights for optimizing pre-\\ntrained models for downstream tasks.\\n7.1\\nSelecting Optimal Weight Matrices for LoRA\\nTo optimize performance under a limited parameter budget, we explore adapting\\ndifferent weight matrices within the self-attention module of the Transformer.\\n10\\n'),\n",
       " Document(metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WgM1DaUn2SYPcCg_It57tA/A-Comprehensive-Review-of-Low-Rank-Adaptation-in-Large-Language-Models-for-Efficient-Parameter-Tuning-1.pdf', 'file_path': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WgM1DaUn2SYPcCg_It57tA/A-Comprehensive-Review-of-Low-Rank-Adaptation-in-Large-Language-Models-for-Efficient-Parameter-Tuning-1.pdf', 'page': 10, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'TeX', 'producer': 'pdfTeX-1.40.26', 'creationDate': 'D:20240910215042Z', 'modDate': 'D:20240910215042Z', 'trapped': ''}, page_content='We allocate 18M parameters (approximately 35MB stored in FP16) for GPT-3\\n175B, using a rank r = 8 for one attention weight type or r = 4 for two types.\\nThe results are displayed in Table 3.\\nTable 3: Validation accuracy on WikiSQL and MultiNLI with LoRA applied to\\ndifferent attention weights in GPT-3, with a fixed number of trainable parame-\\nters.\\n# of Trainable Parameters = 18M\\nWq\\nWk\\nWv\\nWo\\nWq, Wv\\nRank r = 8\\n70.4\\n70.0\\n73.0\\n73.2\\n73.7\\nMultiNLI (±0.1%)\\n91.0\\n90.8\\n91.0\\n91.3\\n91.7\\n7.2\\nDetermining the Ideal Rank for LoRA\\nTo analyze the impact of the rank r on task performance, we applied LoRA with\\nvarying ranks across different combinations of attention matrices. The results\\ncan be found in Table 4.\\nTable 4: Validation accuracy on WikiSQL and MultiNLI with different ranks r.\\nWeight Type\\nr = 1\\nr = 2\\nr = 4\\nr = 8\\nr = 64\\nWikiSQL (±0.5%) Wq\\n68.8\\n69.6\\n70.5\\n70.4\\n70.0\\nWikiSQL (Wq, Wv)\\n73.4\\n73.3\\n73.7\\n73.8\\n73.5\\nMultiNLI (±0.1%) Wq\\n90.7\\n90.9\\n91.1\\n90.7\\n90.7\\nMultiNLI (Wq, Wv)\\n91.3\\n91.4\\n91.3\\n91.6\\n91.4\\nThe results show that even at a small rank r = 1, LoRA performs well when\\nboth Wq and Wv are adapted. In contrast, adapting only Wq requires a higher\\nrank for optimal performance.\\n8\\nConclusion\\nLoRA offers a highly efficient solution to the problem of adapting large language\\nmodels for downstream tasks. By freezing the majority of the model’s param-\\neters and training only small, low-rank matrices, LoRA achieves comparable\\nperformance to full fine-tuning while drastically reducing computational costs.\\nIts ability to scale to massive models like GPT-3 without sacrificing performance\\nhighlights its potential for widespread use.\\nFuture work could explore combining LoRA with other parameter-efficient\\nmethods or investigating more principled ways to select which weight matrices\\nto adapt. Additionally, further studies on the rank deficiency of pre-trained\\nweights could inspire new developments in efficient model adaptation.\\n11\\n')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the PDF Loader\n",
    "loader = PyMuPDFLoader(pdf_url)\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d52201fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print number of pages\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7618cc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Comprehensive Review of Low-Rank\n",
      "Adaptation in Large Language Models for\n",
      "Efficient Parameter Tuning\n",
      "September 10, 2024\n",
      "Abstract\n",
      "Natural Language Processing (NLP) often involves pre-training large\n",
      "models on extensive datasets and then adapting them for specific tasks\n",
      "through fine-tuning. However, as these models grow larger, like GPT-3\n",
      "with 175 billion parameters, fully fine-tuning them becomes computa-\n",
      "tionally expensive. We propose a novel method called LoRA (Low-Rank\n",
      "Adaptation) that significantly reduces the overhead by freezing the orig-\n",
      "inal model weights and only training small rank decomposition matrices.\n",
      "This leads to up to 10,000 times fewer trainable parameters and reduces\n",
      "GPU memory usage by three times. LoRA not only maintains but some-\n",
      "times surpasses fine-tuning performance on models like RoBERTa, De-\n",
      "BERTa, GPT-2, and GPT-3.\n",
      "Unlike other methods, LoRA introduces\n",
      "no extra latency during inference, making it more efficient for practical\n",
      "applications.\n",
      "All relevant code and model checkpoints are available at\n",
      "https://github.com/microsoft/LoRA.\n",
      "1\n",
      "Introduction\n",
      "Many natural language processing (NLP) applications rely on adapting large,\n",
      "pre-trained language models for various downstream tasks. Typically, this is\n",
      "done through fine-tuning, where all the parameters of the pre-trained model are\n",
      "updated. However, a significant drawback of fine-tuning is that the adapted\n",
      "model has just as many parameters as the original one. As models grow in size,\n",
      "what was once a manageable issue for models like GPT-2 or RoBERTa large\n",
      "becomes a serious deployment challenge with larger models like GPT-3, which\n",
      "has 175 billion trainable parameters.\n",
      "To mitigate these challenges, researchers have explored adapting only cer-\n",
      "tain parts of the model or adding external modules specific to each task. This\n",
      "approach reduces the need to store and manage large numbers of parameters\n",
      "for each task, greatly improving efficiency during deployment. However, cur-\n",
      "rent methods often introduce drawbacks, such as inference delays by increasing\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the contents of very first page\n",
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f2d3333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Comprehensive Review of Low-Rank\n",
      "Adaptation in Large Language Models for\n",
      "Efficient Parameter Tuning\n",
      "September 10, 2024\n",
      "Abstract\n",
      "Natural Language Processing (NLP) often involves pre-training large\n",
      "models on extensive datasets and then adapting them for specific tasks\n",
      "through fine-tuning. However, as these models grow larger, like GPT-3\n",
      "with 175 billion parameters, fully fine-tuning them becomes computa-\n",
      "tionally expensive. We propose a novel method called LoRA (Low-Rank\n",
      "Adaptation) that significantly reduces the overhead by freezing the orig-\n",
      "inal model weights and only training small rank decomposition matrices.\n",
      "This leads to up to 10,000 times fewer trainable parameters and reduces\n",
      "GPU memory usage by three times. LoRA not only maintains but some-\n",
      "times surpasses fine-tuning performance on models like RoBERTa, De-\n",
      "BERTa, GPT-2, and GPT-3.\n",
      "Unlike other methods, LoRA introduces\n",
      "no extra latency during inference, making it more efficient for practical\n",
      "applications.\n",
      "All relevant code an\n"
     ]
    }
   ],
   "source": [
    "# Print first 1000 characters of the pdf\n",
    "print(data[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0037db",
   "metadata": {},
   "source": [
    "## SCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a44b059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Comprehensive Review of Low-Rank\n",
      "Adaptation in Large Language Models for\n",
      "Efficient Parameter Tuning\n",
      "September 10, 2024\n",
      "Abstract\n",
      "Natural Language Processing (NLP) often involves pre-training large\n",
      "models on extensive datasets and then adapting them for specific tasks\n",
      "through fine-tuning. However, as these models grow larger, like GPT-3\n",
      "with 175 billion parameters, fully fine-tuning them becomes computa-\n",
      "tionally expensive. We propose a novel method called LoRA (Low-Rank\n",
      "Adaptation) that significantly reduces the overhead by freezing the orig-\n",
      "inal model weights and only training small rank decomposition matrices.\n",
      "This leads to up to 10,000 times fewer trainable parameters and reduces\n",
      "GPU memory usage by three times. LoRA not only maintains but some-\n",
      "times surpasses fine-tuning performance on models like RoBERTa, De-\n",
      "BERTa, GPT-2, and GPT-3.\n",
      "Unlike other methods, LoRA introduces\n",
      "no extra latency during inference, making it more efficient for practical\n",
      "applications.\n",
      "All relevant code an\n"
     ]
    }
   ],
   "source": [
    "# PDF URL\n",
    "pdf_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WgM1DaUn2SYPcCg_It57tA/A-Comprehensive-Review-of-Low-Rank-Adaptation-in-Large-Language-Models-for-Efficient-Parameter-Tuning-1.pdf\"\n",
    "\n",
    "# Load the PDF Loader\n",
    "loader = PyMuPDFLoader(pdf_url)\n",
    "data = loader.load()\n",
    "\n",
    "# Print first 1000 characters of the pdf\n",
    "print(data[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c2f59c",
   "metadata": {},
   "source": [
    "# **Task 2: Apply text splitting techniques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dd73da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cpp',\n",
       " 'go',\n",
       " 'java',\n",
       " 'kotlin',\n",
       " 'js',\n",
       " 'ts',\n",
       " 'php',\n",
       " 'proto',\n",
       " 'python',\n",
       " 'rst',\n",
       " 'ruby',\n",
       " 'rust',\n",
       " 'scala',\n",
       " 'swift',\n",
       " 'markdown',\n",
       " 'latex',\n",
       " 'html',\n",
       " 'sol',\n",
       " 'csharp',\n",
       " 'cobol',\n",
       " 'c',\n",
       " 'lua',\n",
       " 'perl',\n",
       " 'haskell',\n",
       " 'elixir',\n",
       " 'powershell']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.value for e in Language]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67ca2a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LATEX code\n",
    "latex_text = \"\"\"\n",
    "\\documentclass{article}\n",
    "\n",
    "\\begin{document}\n",
    "\n",
    "\\maketitle\n",
    "\n",
    "\\section{Introduction}\n",
    "Large language models (LLMs) are a type of machine learning model that can be trained on vast amounts of text data to generate human-like language. In recent years, LLMs have made significant advances in various natural language processing tasks, including language translation, text generation, and sentiment analysis.\n",
    "\n",
    "\\subsection{History of LLMs}\n",
    "The earliest LLMs were developed in the 1980s and 1990s, but they were limited by the amount of data that could be processed and the computational power available at the time. In the past decade, however, advances in hardware and software have made it possible to train LLMs on massive datasets, leading to significant improvements in performance.\n",
    "\n",
    "\\subsection{Applications of LLMs}\n",
    "LLMs have many applications in the industry, including chatbots, content creation, and virtual assistants. They can also be used in academia for research in linguistics, psychology, and computational linguistics.\n",
    "\n",
    "\\end{document}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70953290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\\\documentclass{article}\\n\\n\\x08egin{document}\\n\\n\\\\maketitle'),\n",
       " Document(page_content='\\\\section{Introduction}\\nLarge language models (LLMs) are a'),\n",
       " Document(page_content='type of machine learning model that can be trained on vast'),\n",
       " Document(page_content='amounts of text data to generate human-like language. In'),\n",
       " Document(page_content='recent years, LLMs have made significant advances in'),\n",
       " Document(page_content='various natural language processing tasks, including'),\n",
       " Document(page_content='language translation, text generation, and sentiment'),\n",
       " Document(page_content='analysis.'),\n",
       " Document(page_content='\\\\subsection{History of LLMs}\\nThe earliest LLMs were'),\n",
       " Document(page_content='developed in the 1980s and 1990s, but they were limited by'),\n",
       " Document(page_content='the amount of data that could be processed and the'),\n",
       " Document(page_content='computational power available at the time. In the past'),\n",
       " Document(page_content='decade, however, advances in hardware and software have'),\n",
       " Document(page_content='made it possible to train LLMs on massive datasets, leading'),\n",
       " Document(page_content='to significant improvements in performance.'),\n",
       " Document(page_content='\\\\subsection{Applications of LLMs}\\nLLMs have many'),\n",
       " Document(page_content='applications in the industry, including chatbots, content'),\n",
       " Document(page_content='creation, and virtual assistants. They can also be used in'),\n",
       " Document(page_content='academia for research in linguistics, psychology, and'),\n",
       " Document(page_content='computational linguistics.\\n\\n\\\\end{document}')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Latex code splitter\n",
    "latex_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.LATEX, chunk_size = 60, chunk_overlap=0)\n",
    "latex_docs = latex_splitter.create_documents([latex_text])\n",
    "latex_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d99d867",
   "metadata": {},
   "source": [
    "# **Task 3: Embed documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "daa3f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_params = {\n",
    "    EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3,\n",
    "    EmbedTextParamsMetaNames.RETURN_OPTIONS: {\"input_text\": True},\n",
    "}\n",
    "\n",
    "watsonx_embedding = WatsonxEmbeddings(\n",
    "    model_id=\"ibm/slate-125m-english-rtrvr\",\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    project_id=os.getenv(\"PROJECT_ID\"),\n",
    "    params=embed_params,\n",
    "    apikey=os.getenv(\"API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "948cc36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.06722454, -0.023729993, 0.017487843, -0.013195328, -0.039584607]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How are you?\"\n",
    "query_result = watsonx_embedding.embed_query(query)\n",
    "query_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "536aa7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f1efa2",
   "metadata": {},
   "source": [
    "# **Task 4: Create and configure vector databases to store embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4210dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Code of Conduct\n",
      "\n",
      "Our Code of Conduct establishes the core values and ethical standards that all members of our organization must adhere to. We are committed to fostering a workplace characterized by integrity, respect, and accountability.\n",
      "\n",
      "Integrity: We commit to the highest ethical standards by being honest and transparent in all our dealings, whether with colleagues, clients, or the community. We protect sensitive information and avoid conflicts of interest.\n",
      "\n",
      "Respect: We value diversity and every individual's contribution. Discrimination, harassment, or any form of disrespect is not tolerated. We promote an inclusive environment where differences are respected, and everyone is treated with dignity.\n",
      "\n",
      "Accountability: We are responsible for our actions and decisions, complying with all relevant laws and regulations. We aim for continuous improvement and report any breaches of this code, supporting investigations into such matters.\n",
      "\n",
      "Safety: We prioritize the safety of our employees, clients, and the community. We encourage a culture of safety, including reporting any unsafe practices or conditions.\n",
      "\n",
      "Environmental Responsibility: We strive to reduce our environmental impact and promote sustainable practices.\n",
      "\n",
      "This Code of Conduct is the cornerstone of our organizational culture. We expect every employee to uphold these principles and act as role models, ensuring our reputation for ethical conduct, integrity, and social responsibility.\n",
      "\n",
      "2. Recruitment Policy\n",
      "\n",
      "Our Recruitment Policy is dedicated to attracting, selecting, and integrating the most qualified and diverse candidates into our organization. The success of our company depends on the talent, skills, and commitment of our employees.\n",
      "\n",
      "Equal Opportunity: We are an equal opportunity employer and do not discriminate based on race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or any other protected status. We actively support diversity and inclusion.\n",
      "\n",
      "Transparency: We maintain a transparent recruitment process. Job vacancies are advertised both internally and externally when appropriate. Job descriptions and requirements are clear and accurately reflect the role.\n",
      "\n",
      "Selection Criteria: We base our selection on qualifications, experience, and skills relevant to the role. Our interviews and assessments are objective, and decisions are made impartially.\n",
      "\n",
      "Data Privacy: We are dedicated to protecting candidates' personal information and comply with all applicable data protection laws.\n",
      "\n",
      "Feedback: Candidates receive timely and constructive feedback on their applications and interview performance.\n",
      "\n",
      "Onboarding: New hires receive thorough onboarding to help them integrate effectively, including an overview of our culture, policies, and expectations.\n",
      "\n",
      "Employee Referrals: We welcome employee referrals as they help build a strong and engaged team.\n",
      "\n",
      "This policy lays the foundation for a diverse, inclusive, and talented workforce. It ensures that we hire candidates who align with our values and contribute to our success. We regularly review and update this policy to incorporate best practices in recruitment.\n",
      "\n",
      "\n",
      "3. Internet and Email Policy\n",
      "\n",
      "Our Internet and Email Policy ensures the responsible and secure use of these tools within our organization, recognizing their importance in daily operations and the need for compliance with security, productivity, and legal standards.\n",
      "\n",
      "Acceptable Use: Company-provided internet and email are primarily for job-related tasks. Limited personal use is permitted during non-work hours as long as it does not interfere with work duties.\n",
      "\n",
      "Security: Protect your login credentials and avoid sharing passwords. Be cautious with email attachments and links from unknown sources, and promptly report any unusual online activity or potential security threats.\n",
      "\n",
      "Confidentiality: Use email for confidential information, trade secrets, and sensitive customer data only with encryption. Be careful when discussing company matters on public platforms or social media.\n",
      "\n",
      "Harassment and Inappropriate Content: Internet and email must not be used for harassment, discrimination, or the distribution of offensive content. Always communicate respectfully and sensitively online.\n",
      "\n",
      "Compliance: Adhere to all relevant laws and regulations concerning internet and email use, including copyright and data protection laws.\n",
      "\n",
      "Monitoring: The company reserves the right to monitor internet and email usage for security and compliance purposes.\n",
      "\n",
      "Consequences: Violations of this policy may lead to disciplinary action, including potential termination.\n",
      "\n",
      "This policy promotes the safe and responsible use of digital communication tools in line with our values and legal obligations. Employees must understand and comply with this policy. Regular reviews will ensure it remains relevant with changing technology and security standards.\n",
      "\n",
      "4. Mobile Phone Policy\n",
      "\n",
      "Our Mobile Phone Policy defines standards for responsible use of mobile devices within the organization to ensure alignment with company values and legal requirements.\n",
      "\n",
      "Acceptable Use: Mobile devices are primarily for work-related tasks. Limited personal use is allowed if it does not disrupt work responsibilities.\n",
      "\n",
      "Security: Secure your mobile device and credentials. Be cautious with app downloads and links from unknown sources, and report any security issues promptly.\n",
      "\n",
      "Confidentiality: Avoid sharing sensitive company information via unsecured messaging apps or emails. Exercise caution when discussing company matters in public.\n",
      "\n",
      "Cost Management: Personal use of mobile phones should be separate from company accounts, and any personal charges on company-issued phones must be reimbursed.\n",
      "\n",
      "Compliance: Comply with all relevant laws and regulations concerning mobile phone usage, including data protection and privacy laws.\n",
      "\n",
      "Lost or Stolen Devices: Immediately report any lost or stolen mobile devices to the IT department or your supervisor.\n",
      "\n",
      "Consequences: Non-compliance with this policy may result in disciplinary actions, including potential loss of mobile phone privileges.\n",
      "\n",
      "This policy encourages the responsible use of mobile devices in line with legal and ethical standards. Employees are expected to understand and follow these guidelines. The policy is regularly reviewed to stay current with evolving technology and security best practices.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the Text Loader\n",
    "txt_loader = TextLoader(\"new-Policies.txt\")\n",
    "txt_data = txt_loader.load()\n",
    "print(txt_data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e51d425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'new-Policies.txt'}, page_content='1. Code of Conduct'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Our Code of Conduct establishes the core values and ethical standards that all members of our'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='all members of our organization must adhere to. We are committed to fostering a workplace'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='a workplace characterized by integrity, respect, and accountability.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Integrity: We commit to the highest ethical standards by being honest and transparent in all our'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='in all our dealings, whether with colleagues, clients, or the community. We protect sensitive'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='protect sensitive information and avoid conflicts of interest.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content=\"Respect: We value diversity and every individual's contribution. Discrimination, harassment, or any\"),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='harassment, or any form of disrespect is not tolerated. We promote an inclusive environment where'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='environment where differences are respected, and everyone is treated with dignity.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Accountability: We are responsible for our actions and decisions, complying with all relevant laws'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='all relevant laws and regulations. We aim for continuous improvement and report any breaches of'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='any breaches of this code, supporting investigations into such matters.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Safety: We prioritize the safety of our employees, clients, and the community. We encourage a'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='We encourage a culture of safety, including reporting any unsafe practices or conditions.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Environmental Responsibility: We strive to reduce our environmental impact and promote sustainable'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='promote sustainable practices.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='This Code of Conduct is the cornerstone of our organizational culture. We expect every employee to'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='every employee to uphold these principles and act as role models, ensuring our reputation for'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='our reputation for ethical conduct, integrity, and social responsibility.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='2. Recruitment Policy'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Our Recruitment Policy is dedicated to attracting, selecting, and integrating the most qualified'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='the most qualified and diverse candidates into our organization. The success of our company depends'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='our company depends on the talent, skills, and commitment of our employees.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Equal Opportunity: We are an equal opportunity employer and do not discriminate based on race,'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='based on race, color, religion, sex, sexual orientation, gender identity, national origin, age,'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='origin, age, disability, or any other protected status. We actively support diversity and'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='diversity and inclusion.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Transparency: We maintain a transparent recruitment process. Job vacancies are advertised both'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='are advertised both internally and externally when appropriate. Job descriptions and requirements'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='and requirements are clear and accurately reflect the role.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Selection Criteria: We base our selection on qualifications, experience, and skills relevant to the'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='relevant to the role. Our interviews and assessments are objective, and decisions are made'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='decisions are made impartially.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content=\"Data Privacy: We are dedicated to protecting candidates' personal information and comply with all\"),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='and comply with all applicable data protection laws.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Feedback: Candidates receive timely and constructive feedback on their applications and interview'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='and interview performance.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Onboarding: New hires receive thorough onboarding to help them integrate effectively, including an'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='including an overview of our culture, policies, and expectations.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Employee Referrals: We welcome employee referrals as they help build a strong and engaged team.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='This policy lays the foundation for a diverse, inclusive, and talented workforce. It ensures that'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='It ensures that we hire candidates who align with our values and contribute to our success. We'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='to our success. We regularly review and update this policy to incorporate best practices in'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='best practices in recruitment.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='3. Internet and Email Policy'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Our Internet and Email Policy ensures the responsible and secure use of these tools within our'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='tools within our organization, recognizing their importance in daily operations and the need for'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='and the need for compliance with security, productivity, and legal standards.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Acceptable Use: Company-provided internet and email are primarily for job-related tasks. Limited'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='tasks. Limited personal use is permitted during non-work hours as long as it does not interfere'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='does not interfere with work duties.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Security: Protect your login credentials and avoid sharing passwords. Be cautious with email'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='cautious with email attachments and links from unknown sources, and promptly report any unusual'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='report any unusual online activity or potential security threats.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Confidentiality: Use email for confidential information, trade secrets, and sensitive customer data'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='customer data only with encryption. Be careful when discussing company matters on public platforms'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='on public platforms or social media.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Harassment and Inappropriate Content: Internet and email must not be used for harassment,'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='for harassment, discrimination, or the distribution of offensive content. Always communicate'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Always communicate respectfully and sensitively online.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Compliance: Adhere to all relevant laws and regulations concerning internet and email use,'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='and email use, including copyright and data protection laws.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Monitoring: The company reserves the right to monitor internet and email usage for security and'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='for security and compliance purposes.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Consequences: Violations of this policy may lead to disciplinary action, including potential'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='including potential termination.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='This policy promotes the safe and responsible use of digital communication tools in line with our'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='in line with our values and legal obligations. Employees must understand and comply with this'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='comply with this policy. Regular reviews will ensure it remains relevant with changing technology'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='changing technology and security standards.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='4. Mobile Phone Policy'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Our Mobile Phone Policy defines standards for responsible use of mobile devices within the'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='devices within the organization to ensure alignment with company values and legal requirements.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Acceptable Use: Mobile devices are primarily for work-related tasks. Limited personal use is'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='personal use is allowed if it does not disrupt work responsibilities.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Security: Secure your mobile device and credentials. Be cautious with app downloads and links from'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='and links from unknown sources, and report any security issues promptly.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Confidentiality: Avoid sharing sensitive company information via unsecured messaging apps or'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='messaging apps or emails. Exercise caution when discussing company matters in public.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Cost Management: Personal use of mobile phones should be separate from company accounts, and any'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='accounts, and any personal charges on company-issued phones must be reimbursed.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Compliance: Comply with all relevant laws and regulations concerning mobile phone usage, including'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='usage, including data protection and privacy laws.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Lost or Stolen Devices: Immediately report any lost or stolen mobile devices to the IT department'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='the IT department or your supervisor.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Consequences: Non-compliance with this policy may result in disciplinary actions, including'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='actions, including potential loss of mobile phone privileges.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='This policy encourages the responsible use of mobile devices in line with legal and ethical'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='legal and ethical standards. Employees are expected to understand and follow these guidelines. The'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='guidelines. The policy is regularly reviewed to stay current with evolving technology and security'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='and security best practices.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(txt_data)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b9b0235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bdc77b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [str(i) for i in range(0, len(chunks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81c84569",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb = Chroma.from_documents(chunks, watsonx_embedding, ids=ids)\n",
    "vectordb._collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ac24eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionGetEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': ['0'], 'embeddings': None, 'metadatas': [{'source': 'new-Policies.txt'}], 'documents': ['1. Code of Conduct'], 'uris': None, 'data': None}\n",
      "{'ids': ['1'], 'embeddings': None, 'metadatas': [{'source': 'new-Policies.txt'}], 'documents': ['Our Code of Conduct establishes the core values and ethical standards that all members of our'], 'uris': None, 'data': None}\n",
      "{'ids': ['2'], 'embeddings': None, 'metadatas': [{'source': 'new-Policies.txt'}], 'documents': ['all members of our organization must adhere to. We are committed to fostering a workplace'], 'uris': None, 'data': None}\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(vectordb._collection.get(ids=str(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69d11016",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'new-Policies.txt'}, page_content='This policy encourages the responsible use of mobile devices in line with legal and ethical'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='This policy promotes the safe and responsible use of digital communication tools in line with our'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='This policy lays the foundation for a diverse, inclusive, and talented workforce. It ensures that'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Environmental Responsibility: We strive to reduce our environmental impact and promote sustainable'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='guidelines. The policy is regularly reviewed to stay current with evolving technology and security')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Smoking Policy\"\n",
    "docs = vectordb.similarity_search(query, k=5)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c20fff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionDeleteEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    }
   ],
   "source": [
    "ids = vectordb.get()[\"ids\"]\n",
    "vectordb.delete(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a017de3",
   "metadata": {},
   "source": [
    "## SCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d6f96c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'new-Policies.txt'}, page_content='This policy encourages the responsible use of mobile devices in line with legal and ethical'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='This policy promotes the safe and responsible use of digital communication tools in line with our'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='This policy lays the foundation for a diverse, inclusive, and talented workforce. It ensures that'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='guidelines. The policy is regularly reviewed to stay current with evolving technology and security'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='We encourage a culture of safety, including reporting any unsafe practices or conditions.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Text Loader\n",
    "txt_loader = TextLoader(\"new-Policies.txt\")\n",
    "txt_data = txt_loader.load()\n",
    "\n",
    "# Text Splitting\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(txt_data)\n",
    "\n",
    "# Create Chroma Vector Database\n",
    "vectordb = Chroma.from_documents(chunks, watsonx_embedding)\n",
    "\n",
    "# Conduct a similarity search and retrieve top 5 results\n",
    "query = \"Smoking policy\"\n",
    "docs = vectordb.similarity_search(query, k=5)\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19314ccd",
   "metadata": {},
   "source": [
    "# **Task 5: Develop a retriever to fetch document segments based on queries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83f4255a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'new-Policies.txt'}, page_content='and email use, including copyright and data protection laws.'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='This policy encourages the responsible use of mobile devices in line with legal and ethical')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\":2})\n",
    "query = \"Email policy\"\n",
    "docs = retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c69c12",
   "metadata": {},
   "source": [
    "# **Task 6: Construct a QA Bot that leverages the LangChain and LLM to answer questions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "728eb250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_model(model_id):\n",
    "    parameters = {\n",
    "        GenParams.MAX_NEW_TOKENS: 256,  # this controls the maximum number of tokens in the generated output\n",
    "        GenParams.TEMPERATURE: 0.5, # this randomness or creativity of the model's responses\n",
    "    }\n",
    "    \n",
    "    credentials = {\n",
    "        \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "        \"api_key\": os.getenv(\"API_KEY\")\n",
    "    }\n",
    "    \n",
    "    project_id = os.getenv(\"PROJECT_ID\")\n",
    "    \n",
    "    model = ModelInference(\n",
    "        model_id=model_id,\n",
    "        params=parameters,\n",
    "        credentials=credentials,\n",
    "        project_id=project_id\n",
    "    )\n",
    "    \n",
    "    llm = WatsonxLLM(watsonx_model = model)\n",
    "    \n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e773f8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ynaya\\OneDrive\\Documents\\RAG Assistant\\venv\\Lib\\site-packages\\ibm_watsonx_ai\\foundation_models\\utils\\utils.py:370: LifecycleWarning: Model 'mistralai/mistral-large' is in deprecated state from 2025-07-09 until 2025-10-08. IDs of alternative models: mistralai/mistral-medium-2505. Further details: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-lifecycle.html?context=wx&audience=wdp\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mixtral_llm = llm_model(\"mistralai/mistral-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ee82467",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_doc = \"\"\n",
    "glob_vectordb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a415c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever_qa(file, query):\n",
    "\n",
    "    global loaded_doc\n",
    "    \n",
    "    if file.name != loaded_doc:\n",
    "\n",
    "        ### print(\"New file detected\")\n",
    "\n",
    "        loaded_doc = file.name\n",
    "        \n",
    "        ### Load the PDF Loader\n",
    "        loader = PyMuPDFLoader(file)\n",
    "        data = loader.load()\n",
    "\n",
    "        ### Text Splitting\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=50,\n",
    "            length_function=len\n",
    "        )\n",
    "\n",
    "        chunks = text_splitter.split_documents(data)\n",
    "\n",
    "        ids = [str(i) for i in range(0, len(chunks))]\n",
    "\n",
    "        ### Create Chroma Vector Database\n",
    "        vectordb1 = Chroma.from_documents(chunks, watsonx_embedding, ids=ids)\n",
    "\n",
    "        global glob_vectordb \n",
    "        glob_vectordb = vectordb1\n",
    "\n",
    "        ### print(glob_vectordb._collection.count())\n",
    "\n",
    "    ## QA Bot\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm = mixtral_llm,\n",
    "        chain_type = \"stuff\",\n",
    "        retriever = glob_vectordb.as_retriever(),\n",
    "        return_source_documents = False\n",
    "    )\n",
    "\n",
    "    ## Generated output\n",
    "    response = qa.invoke(query)[\"result\"]\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c760b901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ynaya\\OneDrive\\Documents\\RAG Assistant\\venv\\Lib\\site-packages\\gradio\\analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.44.0, however version 4.44.1 is available, please upgrade. \n",
      "--------\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    }
   ],
   "source": [
    "# Gradio interface\n",
    "rag_application = gr.Interface(\n",
    "    fn=retriever_qa,\n",
    "    allow_flagging=\"never\",\n",
    "    inputs=[\n",
    "        gr.File(label=\"Upload PDF File\", file_count=\"single\", file_types=['.pdf'], type=\"filepath\"),\n",
    "        gr.Textbox(label=\"Input Query\", lines=2, placeholder=\"Type your question here...\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Output\"),\n",
    "    title=\"AI RAG Assistant\",\n",
    "    description=\"Upload a PDF document and ask any question. The chatbot will try to answer using the provided document.\"\n",
    ")\n",
    "\n",
    "rag_application.launch(server_name=\"0.0.0.0\", server_port= 7860)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
